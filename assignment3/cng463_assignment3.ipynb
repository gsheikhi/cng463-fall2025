{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3Qjh8mRHqD"
      },
      "source": [
        "# Assignment 3: Hidden Markov Models for POS Tagging and Text Generation\n",
        "## CNG463 - Introduction to Natural Language Processing\n",
        "### METU NCC Computer Engineering | Fall 2025-26\n",
        "\n",
        "**Student Name:**  \n",
        "**Student ID:**  \n",
        "**Due Date:** 14 December 2025 (Sunday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDEnwcxRHqE"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This assignment focuses on:\n",
        "1. Building **supervised**, **unsupervised**, and **semi-supervised** HMM models for Part-of-Speech (POS) tagging\n",
        "2. Implementing **5-fold cross-validation** to evaluate model performance\n",
        "3. Comparing the three training approaches using per-tag and overall accuracy\n",
        "4. Using HMMs for **creative text generation** with temperature-based sampling\n",
        "\n",
        "**Note:** You will use the Brown corpus with universal POS tagset. Start with 5000 sentences for debugging, then run on the full corpus (or as much as Colab can handle).\n",
        "\n",
        "**Grading:**\n",
        "- Helper Functions: **10 pts**\n",
        "  - `remove_tags()`: 2 pts\n",
        "  - `evaluate_tagger()`: 5 pts\n",
        "  - Results display: 3 pts\n",
        "- Semi-supervised HMM: **20 pts**\n",
        "- 5-fold cross-validation: **25 pts**\n",
        "  - Supervised HMM: 10 pts\n",
        "  - Unsupervised HMM (Baum-Welch): 10 pts\n",
        "  - Semi-supervised HMM\n",
        "  - Evaluate and Accumulate Results: 5 pts\n",
        "- Report on Results: **5 pts**\n",
        "- Text Generation: **24 pts**\n",
        "  - `sample_state()`: 7 pts\n",
        "  - `sample_word()`: 7 pts\n",
        "  - `generate_text_from_word()`: 10 pts\n",
        "- Written Questions (4 √ó 4 pts): **16 pts**\n",
        "- **Total: 100 pts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srF42PZXRHqP"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0Xw3YQRHqE"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwyLNfKaRHqE"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# NLTK for corpus and HMM\n",
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Scikit-learn for cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-UNnAGRHqF"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: HMM-based POS Tagging (72 points)\n",
        "\n",
        "In this part, you will implement three different approaches to training HMM models for POS tagging:\n",
        "1. **Supervised learning**: Uses fully labelled data\n",
        "2. **Unsupervised learning**: Uses unlabelled data with Baum-Welch algorithm\n",
        "3. **Semi-supervised learning**: Combines a small amount of labelled data with larger unlabelled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5k13c4MRHqF"
      },
      "source": [
        "## 1.1: Load the Brown Corpus\n",
        "\n",
        "Load the Brown corpus with universal POS tagset. Start with 5000 sentences for testing, then increase to the maximum your environment can handle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb6tjNLmRHqF"
      },
      "outputs": [],
      "source": [
        "# Download required NLTK data\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "NUM_SENTENCES = 5000  # Start with 5000 sentences and increase later\n",
        "\n",
        "all_data = list(brown.tagged_sents(tagset=\"universal\"))[:5000]\n",
        "\n",
        "print(f\"Total sentences loaded: {len(all_data)}\")\n",
        "print(f\"\\nExample sentence:\")\n",
        "print(all_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDDP9gORHqF"
      },
      "source": [
        "## 1.2: Remove Tags (2 points)\n",
        "\n",
        "Implement the `remove_tags()` function that converts tagged sentences to untagged format required by NLTK's unsupervised training.\n",
        "\n",
        "**Input:** List of tagged sentences `[[(word, tag), ...], ...]`  \n",
        "**Output:** List of untagged sentences `[[(word, None), ...], ...]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI_9lAXTRHqF"
      },
      "outputs": [],
      "source": [
        "def remove_tags(tagged_sents):\n",
        "    \"\"\"\n",
        "    Remove tags from tagged sentences to create untagged data.\n",
        "\n",
        "    Args:\n",
        "        tagged_sents: List of tagged sentences [[(word, tag), ...], ...]\n",
        "\n",
        "    Returns:\n",
        "        List of untagged sentences [[(word, None), ...], ...] (required format for NLTK)\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg18ezdTRHqG"
      },
      "source": [
        "## 1.3: Evaluate Tagger (5 points)\n",
        "\n",
        "Implement the `evaluate_tagger()` function that evaluates a trained tagger on test data and returns per-tag accuracy statistics.\n",
        "\n",
        "**Input:**\n",
        "- `tagger`: Trained HMM tagger\n",
        "- `test_data`: List of tagged test sentences\n",
        "\n",
        "**Output:** Dictionary with per-tag statistics `{tag: {'correct': count, 'total': count}}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLjQTbejRHqG"
      },
      "outputs": [],
      "source": [
        "def evaluate_tagger(tagger, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate tagger and return per-tag accuracy.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        test_data: List of tagged sentences for testing\n",
        "\n",
        "    Returns:\n",
        "        dict of {tag: {'correct': count, 'total': count}}\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Initialize tag_stats as defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "    # 2. For each sentence in test_data:\n",
        "    #    a. Extract words and true tags\n",
        "    #    b. Use tagger.tag(words) to get predicted tags\n",
        "    #    c. Compare true vs predicted tags\n",
        "    #    d. Update tag_stats accordingly\n",
        "    # 3. Handle exceptions (if tagging fails, count all as incorrect)\n",
        "    # 4. Return tag_stats\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkl012"
      },
      "source": [
        "## 1.4: Semi-supervised HMM Training (20 points)\n",
        "\n",
        "Implement semi-supervised HMM training that combines a small amount of labelled data (1%) with larger unlabelled data (99%).\n",
        "\n",
        "**Steps:**\n",
        "1. Split data: 1% tagged, 99% untagged\n",
        "2. Train supervised model on 1% tagged data\n",
        "3. Use this model to initialise Baum-Welch on 99% untagged data\n",
        "4. Refine with unsupervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mno345"
      },
      "outputs": [],
      "source": [
        "def train_semi_supervised(tagged_data, percent_tagged=1.0):\n",
        "    \"\"\"\n",
        "    Train HMM with a mix of tagged and untagged data.\n",
        "\n",
        "    Args:\n",
        "        tagged_data: List of tagged sentences\n",
        "        percent_tagged: Percentage of data to keep tags (default 1.0%)\n",
        "\n",
        "    Returns:\n",
        "        Trained HMM tagger\n",
        "    \"\"\"\n",
        "    # TODO: Implement semi-supervised training\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Calculate split index: int(len(tagged_data) * (percent_tagged / 100.0))\n",
        "    #    Ensure at least 1 sentence is tagged\n",
        "    #\n",
        "    # 2. Split data:\n",
        "    #    tagged_portion = tagged_data[:split_idx]\n",
        "    #    untagged_portion = remove_tags(tagged_data[split_idx:])\n",
        "    #\n",
        "    # 3. Extract states and symbols from ALL data\n",
        "    #    NOTE: In fact, we should have used only the tagged_data for this,\n",
        "    #    but we are not sure if that portion includes all the tags and symbols\n",
        "    #\n",
        "    # 4. Initialize trainer with states and symbols using\n",
        "    #    hmm.HiddenMarkovModelTrainer\n",
        "    #\n",
        "    # 5. Train supervised model on small tagged data using trainer.train_supervised\n",
        "    #\n",
        "    # 6. Refine with Baum-Welch on untagged data using trainer.train_unsupervised\n",
        "    #\n",
        "    # 7. Return refined_tagger (or error if no untagged data or no tagged data)\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqr678"
      },
      "source": [
        "## 1.5: 5-Fold Cross-Validation (25 Points)\n",
        "\n",
        "Implement 5-fold cross-validation to train and evaluate all three models. This ensures robust performance estimates.\n",
        "\n",
        "**Steps:**\n",
        "1. Split data into 5 folds\n",
        "2. For each fold:\n",
        "   - Train\n",
        "    - supervised (`trainer.train_supervised()`)\n",
        "    - unsupervised (`trainer.train_unsupervised()`)\n",
        "    - semi-supervised models (`train_semi_supervised()`)\n",
        "   - Evaluate on test fold\n",
        "   - Accumulate results\n",
        "3. Calculate average accuracy across all folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stu901"
      },
      "outputs": [],
      "source": [
        "# Prepare for 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "# Storage for results across folds\n",
        "results = {\n",
        "    'unsupervised': defaultdict(lambda: {'correct': 0, 'total': 0}),\n",
        "    'supervised': defaultdict(lambda: {'correct': 0, 'total': 0}),\n",
        "    'semi_supervised': defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "}\n",
        "\n",
        "# TODO: Complete the cross-validation loop\n",
        "#\n",
        "# for train_idx, test_idx in kf.split(all_data):\n",
        "#     print(f\"{'='*60}\")\n",
        "#     print(f\"FOLD {fold_num}\")\n",
        "#     print(f\"{'='*60}\")\n",
        "#\n",
        "#     # Split data to train_data and test_data\n",
        "#     ...\n",
        "#\n",
        "#     print(f\"Training set: {len(train_data)} sentences\")\n",
        "#     print(f\"Test set: {len(test_data)} sentences\")\n",
        "#\n",
        "#     # Extract all_tags and all_symbols from training data\n",
        "#      ...\n",
        "#\n",
        "#\n",
        "#     # 1. Unsupervised HMM (Baum-Welch)\n",
        "#     print(\"\\n1. Training Unsupervised HMM (Baum-Welch)...\")\n",
        "#     try:\n",
        "#         1.1 Convert to untagged format\n",
        "#\n",
        "#         1.2 Initialize trainer with states and symbols\n",
        "#\n",
        "#         1.3 Train unsupervised\n",
        "#\n",
        "#         1.4 Evaluate unsupervised tagger and update the results dict\n",
        "#\n",
        "#         print(\"Unsupervised training complete\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Unsupervised training failed: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()\n",
        "#\n",
        "#     # 2. Supervised HMM\n",
        "#     print(\"\\n2. Training Supervised HMM...\")\n",
        "#     try:\n",
        "#         1.1 Initialize trainer with states and symbols\n",
        "#\n",
        "#         1.2 Train supervised\n",
        "#\n",
        "#         1.3 Evaluate supervised tagger and update the results dict\n",
        "#\n",
        "#         print(\"Supervised training complete\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Supervised training failed: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()\n",
        "#\n",
        "#     # 3. Semi-supervised HMM (1% tagged, 99% untagged)\n",
        "#     print(\"\\n3. Training Semi-Supervised HMM (1% tagged, 99% untagged)...\")\n",
        "#     try:\n",
        "#         1.1 Train semi-supervised\n",
        "#\n",
        "#         1.3 Evaluate semi-supervised tagger and update the results dict\n",
        "#\n",
        "#         print(\"Semi-supervised training complete\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"ERROR: Semi-supervised training failed: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZLYOOQPz-Si"
      },
      "source": [
        "**Question 1.1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9EPblW0JgN"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwx234"
      },
      "source": [
        "## 1.6: Display Results (5 points)\n",
        "\n",
        "Display the average across folds results in a formatted table showing\n",
        "- tag frequency (%)\n",
        "- per-tag accuracy (%)\n",
        "- overall accuracy (%)\n",
        "\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "for all three models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yza567"
      },
      "outputs": [],
      "source": [
        "# TODO: Calculate and display results\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd890"
      },
      "source": [
        "## 1.7: Sample Predictions\n",
        "\n",
        "Test the models on sample sentences to see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efg123"
      },
      "outputs": [],
      "source": [
        "# Print sample predictions from the last fold\n",
        "print(\"Sample predictions from last fold:\")\n",
        "sample_sentences = [\n",
        "    \"Today is a good day .\",\n",
        "    \"Joe met Joanne in Delhi .\",\n",
        "    \"Time flies like an arrow .\",\n",
        "    \"The good , the bad , the ugly went to a bar .\"\n",
        "]\n",
        "\n",
        "print(f\"{'='*10} Spervised HMM Tagger {'='*10}\")\n",
        "for sent in sample_sentences:\n",
        "    try:\n",
        "        tagged = tagger_sup.tag(sent.split())\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"  ‚Üí {tagged}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"ERROR: Tagging failed: {e}\\n\")\n",
        "\n",
        "print(f\"{'='*10} Unspervised HMM Tagger {'='*10}\")\n",
        "for sent in sample_sentences:\n",
        "    try:\n",
        "        tagged = tagger_unsup.tag(sent.split())\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"  ‚Üí {tagged}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"ERROR: Tagging failed: {e}\\n\")\n",
        "\n",
        "print(f\"{'='*10} Semi-spervised HMM Tagger {'='*10}\")\n",
        "for sent in sample_sentences:\n",
        "    try:\n",
        "        tagged = tagger_semi_sup.tag(sent.split())\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"  ‚Üí {tagged}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {sent}\")\n",
        "        print(f\"ERROR: Tagging failed: {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question1"
      },
      "source": [
        "**Question 1.2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer1"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question2"
      },
      "source": [
        "**Question 1.3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer2"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Text Generation with HMMs (24 points)\n",
        "\n",
        "In this part, you will use a trained HMM to generate text. The idea is to sample from the learned transition and emission probabilities to create new sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_gen_model"
      },
      "source": [
        "## 2.1: Train HMM Model for Generation\n",
        "\n",
        "First, train a supervised HMM model on the Brown corpus for text generation. We'll normalise words to lowercase for better generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model_gen"
      },
      "outputs": [],
      "source": [
        "def train_hmm_model(num_sentences=5000):\n",
        "    \"\"\"Train an HMM model on Brown corpus for text generation\"\"\"\n",
        "    print(\"Loading Brown corpus...\")\n",
        "    tagged_sents = list(brown.tagged_sents(tagset=\"universal\"))[:num_sentences]\n",
        "\n",
        "    print(f\"Training HMM on {len(tagged_sents)} sentences...\")\n",
        "\n",
        "    # Extract states and symbols\n",
        "    all_tags = set()\n",
        "    all_symbols = set()\n",
        "    for sent in tagged_sents:\n",
        "        for word, tag in sent:\n",
        "            all_tags.add(tag)\n",
        "            all_symbols.add(word.lower())  # Normalise to lowercase\n",
        "\n",
        "    # Normalise training data to lowercase\n",
        "    normalized_sents = []\n",
        "    for sent in tagged_sents:\n",
        "        normalized_sents.append([(word.lower(), tag) for word, tag in sent])\n",
        "\n",
        "    # Train the model\n",
        "    trainer = hmm.HiddenMarkovModelTrainer(\n",
        "        states=list(all_tags),\n",
        "        symbols=list(all_symbols)\n",
        "    )\n",
        "\n",
        "    tagger = trainer.train_supervised(normalized_sents)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return tagger\n",
        "\n",
        "# Train the model\n",
        "tagger_gen = train_hmm_model(num_sentences=20000)\n",
        "\n",
        "# Show model statistics\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  - Number of states (POS tags): {len(tagger_gen._states)}\")\n",
        "print(f\"  - Number of symbols (words): {len(tagger_gen._symbols)}\")\n",
        "print(f\"  - States: {', '.join(sorted(tagger_gen._states))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_state_header"
      },
      "source": [
        "## 2.2: Implement State Sampling (7 points)\n",
        "\n",
        "Implement the `sample_state()` function that samples the next POS tag based on transition probabilities.\n",
        "\n",
        "**Temperature parameter:** Controls randomness\n",
        "- Lower temperature (e.g., 0.5): More conservative, follows high-probability transitions\n",
        "- Higher temperature (e.g., 2.0): More creative, explores diverse transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_state_code"
      },
      "outputs": [],
      "source": [
        "def sample_state(tagger, current_state, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample next state from transition distribution.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        current_state: Current POS tag\n",
        "        temperature: Controls randomness (default 1.0)\n",
        "\n",
        "    Returns:\n",
        "        Next POS tag (state)\n",
        "    \"\"\"\n",
        "    # TODO: Implement state sampling\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Get all possible states from tagger._states\n",
        "    #\n",
        "    # 2. For each state, get transition probability from current_state\n",
        "    #    using: tagger._transitions[current_state].logprob(state)\n",
        "    #\n",
        "    # 3. Apply temperature scaling: probs = probs / temperature\n",
        "    #\n",
        "    # 4. Convert from log2 probabilities to regular probabilities: 2 ** probs\n",
        "    #\n",
        "    # 5. Normalise probabilities to sum to 1\n",
        "    #\n",
        "    # 6. Sample using np.random.choice(states, p=probs)\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_word_header"
      },
      "source": [
        "## 2.3: Implement Word Sampling (7 points)\n",
        "\n",
        "Implement the `sample_word()` function that samples a word given a POS tag based on emission probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_word_code"
      },
      "outputs": [],
      "source": [
        "def sample_word(tagger, state, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample word from output distribution of given state.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        state: Current POS tag\n",
        "        temperature: Controls randomness (default 1.0)\n",
        "\n",
        "    Returns:\n",
        "        Sampled word\n",
        "    \"\"\"\n",
        "    # TODO: Implement word sampling\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Get all possible symbols (words) from tagger._symbols\n",
        "    #\n",
        "    # 2. For each symbol, get emission probability from state\n",
        "    #    using: tagger._outputs[state].logprob(symbol)\n",
        "    #\n",
        "    # 3. Apply temperature scaling: probs = probs / temperature\n",
        "    #\n",
        "    # 4. Convert from log2 probabilities to regular probabilities: 2 ** probs\n",
        "    #\n",
        "    # 5. Handle infinities (words with zero probability) using np.nan_to_num\n",
        "    #\n",
        "    # 6. Normalise probabilities to sum to 1\n",
        "    #    (if all are zero, use uniform distribution)\n",
        "    #\n",
        "    # 7. Sample using np.random.choice(symbols, p=probs)\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gen_text_header"
      },
      "source": [
        "## 2.4: Implement Text Generation (10 points)\n",
        "\n",
        "Implement the `generate_text_from_word()` function that generates text starting from a given word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen_text_code"
      },
      "outputs": [],
      "source": [
        "def generate_text_from_word(tagger, start_word, length=20, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text starting with a given word using the HMM model.\n",
        "\n",
        "    Args:\n",
        "        tagger: Trained HMM tagger\n",
        "        start_word: Word to start the generation\n",
        "        length: Number of words to generate\n",
        "        temperature: Controls randomness (higher = more random)\n",
        "\n",
        "    Returns:\n",
        "        List of generated words\n",
        "    \"\"\"\n",
        "    # TODO: Implement text generation\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Normalise start_word to lowercase\n",
        "    #\n",
        "    # 2. Check if start_word is in vocabulary (tagger._symbols)\n",
        "    #    - If not, find similar words or use random word\n",
        "    #\n",
        "    # 3. Initialise generated list with start_word\n",
        "    #\n",
        "    # 4. Infer the most likely tag for start_word:\n",
        "    #    - For each state, get emission probability\n",
        "    #    - Choose state with highest probability\n",
        "    #\n",
        "    # 5. Generate subsequent words in a loop:\n",
        "    #    - Sample next state using sample_state()\n",
        "    #    - Sample word from that state using sample_word()\n",
        "    #    - Append word to generated list\n",
        "    #    - Update current_state\n",
        "    #\n",
        "    # 6. Return generated list\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_gen_header"
      },
      "source": [
        "## 2.6: Test Text Generation\n",
        "\n",
        "Now test your text generation functions with different starting words and temperatures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_gen_code"
      },
      "outputs": [],
      "source": [
        "# Test with example words\n",
        "print(\"=\"*70)\n",
        "print(\"EXAMPLE GENERATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "example_words = [\"the\", \"dog\", \"running\", \"beautiful\", \"computer\", \"yesterday\"]\n",
        "\n",
        "for word in example_words:\n",
        "    print(f\"\\n--- Starting with '{word}' ---\")\n",
        "    try:\n",
        "        words = generate_text_from_word(tagger_gen, word, length=15, temperature=1.0)\n",
        "        print(\" \".join(words))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_temp_code"
      },
      "outputs": [],
      "source": [
        "# Test different temperatures\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEMPERATURE COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_word = \"the\"\n",
        "temperatures = [(\"Conservative\", 0.5), (\"Balanced\", 1.0), (\"Creative\", 2.0)]\n",
        "\n",
        "for temp_name, temp_value in temperatures:\n",
        "    print(f\"\\n{temp_name} (temp={temp_value}):\")\n",
        "    words = generate_text_from_word(tagger_gen, test_word, length=20, temperature=temp_value)\n",
        "    print(\" \".join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "question3"
      },
      "source": [
        "**Question 2.1:** How does the temperature parameter affect the quality and creativity of generated text? Provide specific examples from your outputs. (4 points, 3-5 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "answer3"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdf_header"
      },
      "source": [
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File ‚Üí Download ‚Üí Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **üìÅ folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "- Wait for the upload to complete\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "\n",
        "### Step 4: Enter Notebook Name\n",
        "- When prompted, type your notebook name (e.g.`gs_000000_as2.ipynb`)\n",
        "- Press Enter\n",
        "\n",
        "### The PDF will automatically download to your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf_code"
      },
      "outputs": [],
      "source": [
        "# # Install required packages (this takes about 30 seconds)\n",
        "# print(\"Installing PDF converter... please wait...\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "# !pip install -q nbconvert\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"COLAB NOTEBOOK TO PDF CONVERTER\")\n",
        "# print(\"=\"*50)\n",
        "# print(\"\\nSTEP 1: Download your notebook\")\n",
        "# print(\"- Go to File ‚Üí Download ‚Üí Download .ipynb\")\n",
        "# print(\"- Save it to your computer\")\n",
        "# print(\"\\nSTEP 2: Upload it here\")\n",
        "# print(\"- Click the folder icon on the left (üìÅ)\")\n",
        "# print(\"- Click the upload button and select your .ipynb file\")\n",
        "# print(\"- Wait for upload to complete\")\n",
        "# print(\"\\nSTEP 3: Enter the filename below\")\n",
        "# print(\"=\"*50)\n",
        "\n",
        "# # Get notebook name from user\n",
        "# notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# # Add .ipynb if missing\n",
        "# if not notebook_name.endswith('.ipynb'):\n",
        "#     notebook_name += '.ipynb'\n",
        "\n",
        "# import os\n",
        "# notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# # Check if file exists\n",
        "# if not os.path.exists(notebook_path):\n",
        "#     print(f\"\\n‚ö† Error: '{notebook_name}' not found in /content/\")\n",
        "#     print(\"\\nMake sure you uploaded the file using the folder icon (üìÅ) on the left!\")\n",
        "# else:\n",
        "#     print(f\"\\n‚úì Found {notebook_name}\")\n",
        "#     print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "#     # Convert the notebook to PDF\n",
        "#     !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "#     # Download the PDF\n",
        "#     from google.colab import files\n",
        "#     pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "#     pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "#     if os.path.exists(pdf_path):\n",
        "#         print(\"‚úì SUCCESS! Downloading your PDF now...\")\n",
        "#         files.download(pdf_path)\n",
        "#         print(\"\\n‚úì Done! Check your downloads folder.\")\n",
        "#     else:\n",
        "#         print(\"‚ö† Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}