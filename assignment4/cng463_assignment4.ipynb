{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh3Qjh8mRHqD"
      },
      "source": [
        "# Assignment 4: Representation Robustness under Noise\n",
        "## CNG463 - Introduction to Natural Language Processing\n",
        "### METU NCC Computer Engineering | Fall 2025-26\n",
        "\n",
        "**Student Name:**  \n",
        "**Student ID:**  \n",
        "**Due Date:** 26 December 2025 (Friday) before midnight\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyDEnwcxRHqE"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This assignment focuses on:\n",
        "1. Building **TF-IDF** and **word embedding-based** document representations\n",
        "2. Implementing **noise injection** (token deletion, stopword manipulation)\n",
        "3. Applying **PCA** for dimensionality reduction\n",
        "4. Building a **cosine similarity-based information retrieval** system\n",
        "5. Evaluating robustness using **Precision@k, Recall@k, and MAP**\n",
        "\n",
        "**Note:** This assignment deliberately avoids deep learning, transformers, and large language models. You will use the BBC News Dataset with approximately 2,200 news articles across five topics.\n",
        "\n",
        "**Grading:**\n",
        "- Data Loading and Preprocessing: **8 pts**\n",
        "- TF-IDF and Word2Vec Representation: **8 pts**\n",
        "- Noise Injection (2 types): **14 pts**\n",
        "- PCA Dimensionality Reduction: **20 pts**\n",
        "- Information Retrieval System: **20 pts**\n",
        "- Evaluation Metrics: **20 pts**\n",
        "- Written Questions (5 √ó 2 pt): **10 pts**\n",
        "- **Total: 100 pts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srF42PZXRHqP"
      },
      "source": [
        "---\n",
        "\n",
        "## Pre-Submission Checklist\n",
        "\n",
        "- [ ] Name and student ID at top\n",
        "- [ ] No cells are added or removed\n",
        "- [ ] All TODO sections completed\n",
        "- [ ] All questions answered\n",
        "- [ ] Code runs without errors\n",
        "- [ ] Results tables included\n",
        "- [ ] Run All before saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0Xw3YQRHqE"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwyLNfKaRHqE"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "\n",
        "# Standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import os\n",
        "\n",
        "# Scikit-learn for TF-IDF and PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Gensim for Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# NLTK for preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Matplotlib for PCA explained variance curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-UNnAGRHqF"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1: Data Loading and Preprocessing (8 points)\n",
        "\n",
        "Load the BBC News Dataset and apply standard preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5k13c4MRHqF"
      },
      "source": [
        "## 1.1: Load the BBC News Dataset (4 points)\n",
        "\n",
        "Download the dataset and load all documents with their category labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb6tjNLmRHqF"
      },
      "outputs": [],
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download BBC News Dataset\n",
        "!wget -q http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\n",
        "!unzip -q bbc-fulltext.zip\n",
        "\n",
        "# TODO: Load all documents from the BBC dataset\n",
        "#\n",
        "# Steps:\n",
        "# 1. Navigate through the 'bbc' directory structure\n",
        "# 2. For each category folder (business, entertainment, politics, sport, tech):\n",
        "#    - Read all text files\n",
        "#    - Store document text and category label\n",
        "# 3. Create lists: documents (raw text) and labels (category names)\n",
        "#\n",
        "# Expected output:\n",
        "# - documents: list of strings (raw document text)\n",
        "# - labels: list of strings (category names)\n",
        "\n",
        "\n",
        "print(f\"Total documents loaded: {len(documents)}\")\n",
        "print(f\"Categories: {set(labels)}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "for cat in sorted(set(labels)):\n",
        "    print(f\"  {cat}: {labels.count(cat)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwDDP9gORHqF"
      },
      "source": [
        "## 1.2: Preprocessing Function (2 points)\n",
        "\n",
        "Implement a preprocessing function that applies tokenisation, lowercasing, and optional stopword removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI_9lAXTRHqF"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "def preprocess_text(text, remove_stopwords=False):\n",
        "    \"\"\"\n",
        "    Preprocess text: tokenise, lowercase, and optionally remove stopwords.\n",
        "    \n",
        "    Args:\n",
        "        text: Raw document text\n",
        "        remove_stopwords: Whether to remove stopwords (default False)\n",
        "    \n",
        "    Returns:\n",
        "        List of processed tokens\n",
        "    \"\"\"\n",
        "    # TODO: Implement preprocessing\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Tokenise using word_tokenize\n",
        "    # 2. Convert to lowercase\n",
        "    # 3. Keep only alphabetic tokens (use str.isalpha())\n",
        "    # 4. If remove_stopwords is True, filter out stopwords\n",
        "    # 5. Return list of processed tokens\n",
        "    \n",
        "    pass\n",
        "\n",
        "# Test preprocessing\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog!\"\n",
        "print(\"Original:\", sample_text)\n",
        "print(\"Processed:\", preprocess_text(sample_text))\n",
        "print(\"Without stopwords:\", preprocess_text(sample_text, remove_stopwords=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg18ezdTRHqG"
      },
      "source": [
        "## 1.3: Apply Preprocessing to All Documents (2 points)\n",
        "\n",
        "Preprocess all documents and store both tokenised and string versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLjQTbejRHqG"
      },
      "outputs": [],
      "source": [
        "# TODO: Preprocess all documents\n",
        "#\n",
        "# Create two versions:\n",
        "# 1. processed_docs: list of token lists (for Word2Vec)\n",
        "# 2. processed_docs_str: list of strings (for TF-IDF)\n",
        "#\n",
        "# Use remove_stopwords=False for now\n",
        "\n",
        "print(f\"Preprocessed {len(processed_docs)} documents\")\n",
        "print(f\"\\nExample preprocessed document (first 20 tokens):\")\n",
        "print(processed_docs[0][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_section"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2: Representation (8 points)\n",
        "\n",
        "Build document representations using TF-IDF and Word2Vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_build"
      },
      "source": [
        "## 2.1: Build TF-IDF Vectors (2 points)\n",
        "\n",
        "Create TF-IDF document vectors using scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfidf_code"
      },
      "outputs": [],
      "source": [
        "# TODO: Build TF-IDF representation\n",
        "#\n",
        "# Steps:\n",
        "# 1. Initialise TfidfVectorizer with appropriate parameters:\n",
        "#    - min_df: minimum document frequency (try 2 or 3)\n",
        "#    - max_df: maximum document frequency (try 0.8 or 0.9)\n",
        "#    - ngram_range: experiment with (1,1) or (1,2)\n",
        "# 2. Fit and transform the processed_docs_str\n",
        "# 3. Store the resulting matrix as tfidf_matrix\n",
        "# 4. Print shape and number of features\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_question"
      },
      "source": [
        "**Question 2.1:** Explain the TF-IDF parameters you chose (min_df, max_df, ngram_range) and why. (2 point, 2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_answer"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2v_train"
      },
      "source": [
        "## 2.2: Train Word2Vec Model (2 points)\n",
        "\n",
        "Train a Word2Vec model on the BBC News corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2v_train_code"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Word2Vec model\n",
        "#\n",
        "# Steps:\n",
        "# 1. Initialise Word2Vec with parameters:\n",
        "#    - vector_size: embedding dimension (try 100 or 200)\n",
        "#    - window: context window size (try 5)\n",
        "#    - min_count: minimum word frequency (try 2)\n",
        "#    - sg: 0 for CBOW, 1 for Skip-gram (try both)\n",
        "#    - seed: use seed variable for reproducibility\n",
        "# 2. Train on processed_docs\n",
        "# 3. Print vocabulary size\n",
        "\n",
        "w2v_model = None  # YOUR CODE HERE\n",
        "\n",
        "print(f\"Word2Vec vocabulary size: {len(w2v_model.wv)}\")\n",
        "print(f\"Vector dimension: {w2v_model.wv.vector_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2v_doc_embed"
      },
      "source": [
        "## 2.3: Create Document Embeddings (4 points)\n",
        "\n",
        "Aggregate word vectors to create document embeddings using simple averaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2v_doc_code"
      },
      "outputs": [],
      "source": [
        "def create_doc_embedding(tokens, w2v_model):\n",
        "    \"\"\"\n",
        "    Create document embedding by averaging word vectors.\n",
        "    \n",
        "    Args:\n",
        "        tokens: List of tokens in document\n",
        "        w2v_model: Trained Word2Vec model\n",
        "    \n",
        "    Returns:\n",
        "        Document embedding vector (numpy array)\n",
        "    \"\"\"\n",
        "    # TODO: Implement document embedding\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Initialise empty list for word vectors\n",
        "    # 2. For each token in the document:\n",
        "    #    - If token is in model vocabulary, add its vector\n",
        "    # 3. If no valid vectors found, return zero vector\n",
        "    # 4. Otherwise, return mean of all word vectors\n",
        "    \n",
        "    pass\n",
        "\n",
        "# TODO: Create document embeddings for all documents\n",
        "\n",
        "# Convert to numpy array\n",
        "doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "print(f\"Document embeddings shape: {doc_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noise_section"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3: Noise Injection (14 points)\n",
        "\n",
        "Implement controlled noise injection to test representation robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noise_deletion"
      },
      "source": [
        "## 3.1: Random Token Deletion (4 points)\n",
        "\n",
        "Implement random token deletion noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noise_del_code"
      },
      "outputs": [],
      "source": [
        "def apply_token_deletion(tokens, deletion_rate=0.1):\n",
        "    \"\"\"\n",
        "    Randomly delete tokens from document.\n",
        "    \n",
        "    Args:\n",
        "        tokens: List of tokens\n",
        "        deletion_rate: Percentage of tokens to delete (default 0.1 = 10%)\n",
        "    \n",
        "    Returns:\n",
        "        List of tokens after deletion\n",
        "    \"\"\"\n",
        "    # TODO: Implement token deletion\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. For each token, generate random number\n",
        "    # 2. Keep token only if random number > deletion_rate\n",
        "    # 3. Ensure at least one token remains\n",
        "    # 4. Return filtered tokens\n",
        "    \n",
        "    pass\n",
        "\n",
        "# Test token deletion\n",
        "sample_tokens = preprocess_text(\"The quick brown fox jumps over the lazy dog\")\n",
        "print(\"Original:\", sample_tokens)\n",
        "print(\"10% deletion:\", apply_token_deletion(sample_tokens, 0.1))\n",
        "print(\"30% deletion:\", apply_token_deletion(sample_tokens, 0.3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noise_stopword"
      },
      "source": [
        "## 3.2: Stopword Manipulation (3 points)\n",
        "\n",
        "Implement stopword removal as another noise type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noise_stop_code"
      },
      "outputs": [],
      "source": [
        "def apply_stopword_removal(tokens):\n",
        "    \"\"\"\n",
        "    Remove all stopwords from document.\n",
        "    \n",
        "    Args:\n",
        "        tokens: List of tokens\n",
        "    \n",
        "    Returns:\n",
        "        List of tokens without stopwords\n",
        "    \"\"\"\n",
        "    # TODO: Implement stopword removal\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Get English stopwords from NLTK\n",
        "    # 2. Filter out tokens that are stopwords\n",
        "    # 3. Ensure at least one token remains\n",
        "    # 4. Return filtered tokens\n",
        "    \n",
        "    pass\n",
        "\n",
        "# Test stopword removal\n",
        "sample_tokens = preprocess_text(\"The quick brown fox jumps over the lazy dog\")\n",
        "print(\"Original:\", sample_tokens)\n",
        "print(\"Without stopwords:\", apply_stopword_removal(sample_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noise_apply"
      },
      "source": [
        "## 3.3: Apply Noise to All Documents (7 points)\n",
        "\n",
        "Create noisy versions of the dataset for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noise_apply_code"
      },
      "outputs": [],
      "source": [
        "# TODO: Create noisy versions of documents\n",
        "#\n",
        "# Create four versions:\n",
        "# 1. noisy_docs_10: 10% token deletion\n",
        "# 2. noisy_docs_20: 20% token deletion\n",
        "# 3. noisy_docs_30: 30% token deletion\n",
        "# 4. noisy_docs_stopword: stopword removal\n",
        "#\n",
        "# For each, store both token list and string versions\n",
        "\n",
        "noisy_versions = {\n",
        "    '10_del': {'tokens': [], 'strings': []},\n",
        "    '20_del': {'tokens': [], 'strings': []},\n",
        "    '30_del': {'tokens': [], 'strings': []},\n",
        "    'stopword': {'tokens': [], 'strings': []}\n",
        "}\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Noisy versions created:\")\n",
        "for name, data in noisy_versions.items():\n",
        "    print(f\"  {name}: {len(data['tokens'])} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pca_section"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 4: PCA Dimensionality Reduction (20 points)\n",
        "\n",
        "Apply PCA to reduce dimensionality of document representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pca_code"
      },
      "outputs": [],
      "source": [
        "# TODO: PCA with variance-threshold‚Äìbased dimensionality selection\n",
        "#\n",
        "# For each representation (TF-IDF and Word Embeddings):\n",
        "# - Plot cumulative explained variance. (8 points)\n",
        "# - Determine the minimum number of components required to retain\n",
        "#   at least 80% and 90% of the variance. (4 points)\n",
        "# - Apply PCA using these two dimensionalities. (4 points)\n",
        "# - Report the exact value of explained variance and store the reduced representations. (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pca_question"
      },
      "source": [
        "**Question 5.1:** How does PCA affect the information retained in TF-IDF compared to Word2Vec representations? Compare the number of components required to retain 80% and 90% of the variance. (2 point, 2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pca_answer"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir_section"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 5: Information Retrieval System (20 points)\n",
        "\n",
        "Build a cosine similarity-based retrieval system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "queries"
      },
      "source": [
        "## 5.1: Define Queries (10 points)\n",
        "\n",
        "Define 10-15 queries targeting different BBC News categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "queries_code"
      },
      "outputs": [],
      "source": [
        "# TODO: Define queries\n",
        "#\n",
        "# Create a list of dictionaries with:\n",
        "# - 'query': 2-5 content words\n",
        "# - 'target_category': primary BBC News category\n",
        "#\n",
        "# Ensure queries cover all five categories\n",
        "\n",
        "queries = [\n",
        "    # Business queries\n",
        "    {'query': 'stock market economy', 'target_category': 'business'},\n",
        "    # Add more queries here\n",
        "    # YOUR CODE HERE\n",
        "]\n",
        "\n",
        "print(f\"Total queries defined: {len(queries)}\")\n",
        "print(\"\\nQueries by category:\")\n",
        "for cat in ['business', 'entertainment', 'politics', 'sport', 'tech']:\n",
        "    count = sum(1 for q in queries if q['target_category'] == cat)\n",
        "    print(f\"  {cat}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "retrieval"
      },
      "source": [
        "## 5.2: Implement Retrieval Function (10 points)\n",
        "\n",
        "Implement cosine similarity-based document retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "retrieval_code"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents(query_text, doc_vectors, vectorizer=None, w2v_model=None, top_k=10):\n",
        "    \"\"\"\n",
        "    Retrieve top-k documents for a query using cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        query_text: Query string\n",
        "        doc_vectors: Document representation matrix\n",
        "        vectorizer: TfidfVectorizer (if using TF-IDF)\n",
        "        w2v_model: Word2Vec model (if using embeddings)\n",
        "        top_k: Number of documents to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        List of document indices ranked by similarity\n",
        "    \"\"\"\n",
        "    # TODO: Implement retrieval\n",
        "    #\n",
        "    # Steps:\n",
        "    # 1. Preprocess query text\n",
        "    # 2. Convert query to same representation as documents:\n",
        "    #    - If vectorizer: use vectorizer.transform\n",
        "    #    - If w2v_model: use create_doc_embedding\n",
        "    # 3. Compute cosine similarity with all documents\n",
        "    # 4. Get top-k document indices by similarity\n",
        "    # 5. Return ranked indices\n",
        "    \n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_section"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 6: Evaluation Metrics (20 points)\n",
        "\n",
        "Implement and compute IR evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metrics_impl"
      },
      "source": [
        "## 6.1: Implement Evaluation Metrics (8 points)\n",
        "\n",
        "Implement Precision@k, Recall@k, and MAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics_code"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
        "    \"\"\"\n",
        "    Calculate Precision@k.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_docs: List of retrieved document indices\n",
        "        relevant_docs: Set of relevant document indices\n",
        "        k: Number of top documents to consider\n",
        "    \n",
        "    Returns:\n",
        "        Precision@k score\n",
        "    \"\"\"\n",
        "    # TODO: Implement Precision@k\n",
        "    pass\n",
        "\n",
        "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
        "    \"\"\"\n",
        "    Calculate Recall@k.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_docs: List of retrieved document indices\n",
        "        relevant_docs: Set of relevant document indices\n",
        "        k: Number of top documents to consider\n",
        "    \n",
        "    Returns:\n",
        "        Recall@k score\n",
        "    \"\"\"\n",
        "    # TODO: Implement Recall@k\n",
        "    pass\n",
        "\n",
        "def average_precision(retrieved_docs, relevant_docs):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision.\n",
        "    \n",
        "    Args:\n",
        "        retrieved_docs: List of retrieved document indices\n",
        "        relevant_docs: Set of relevant document indices\n",
        "    \n",
        "    Returns:\n",
        "        Average Precision score\n",
        "    \"\"\"\n",
        "    # TODO: Implement Average Precision\n",
        "    #\n",
        "    # For each relevant document in retrieved list:\n",
        "    # - Calculate precision at that position\n",
        "    # - Average all precision values\n",
        "    pass\n",
        "\n",
        "def mean_average_precision(queries_results):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision across all queries.\n",
        "    \n",
        "    Args:\n",
        "        queries_results: List of (retrieved_docs, relevant_docs) tuples\n",
        "    \n",
        "    Returns:\n",
        "        MAP score\n",
        "    \"\"\"\n",
        "    # TODO: Implement MAP\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_run"
      },
      "source": [
        "## 6.2: Run Complete Evaluation (12 points)\n",
        "\n",
        "Evaluate all representations (clean and noisy) using all metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_run_code"
      },
      "outputs": [],
      "source": [
        "# TODO: Complete evaluation framework\n",
        "#\n",
        "# For each representation type:\n",
        "# - TF-IDF (original, PCA-80%, PCA-90%)\n",
        "# - Word Embeddings (original, PCA-80%, PCA-90%)\n",
        "#\n",
        "# For each noise condition:\n",
        "# - Clean (no noise)\n",
        "# - 10% deletion\n",
        "# - 20% deletion\n",
        "# - 30% deletion\n",
        "# - Stopword removal\n",
        "#\n",
        "# For each query:\n",
        "# - Retrieve top-10 documents\n",
        "# - Identify relevant documents (same category)\n",
        "# - Calculate Precision@5, Recall@5, AP\n",
        "#\n",
        "# Aggregate results and compute MAP\n",
        "#\n",
        "# Store results in structured format (e.g., DataFrame)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Display results table\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "# Display your results here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis_questions"
      },
      "source": [
        "## 6.3: Analysis Questions\n",
        "\n",
        "Answer the following questions based on your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1"
      },
      "source": [
        "**Question 7.1:** Which representation (TF-IDF or word embeddings) is more robust to token deletion noise? Support your answer with specific metric values. (2 point, 2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2"
      },
      "source": [
        "**Question 7.2:** How does stopword removal affect retrieval performance compared to token deletion? Which noise type is more damaging? (2 point, 2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3"
      },
      "source": [
        "**Question 7.3:** Does PCA improve or hurt robustness to noise? Compare performance with and without PCA under noisy conditions. (2 point, 2-3 sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3"
      },
      "source": [
        "**[YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdf_header"
      },
      "source": [
        "---\n",
        "\n",
        "# Convert Your Colab Notebook to PDF\n",
        "\n",
        "### Step 1: Download Your Notebook\n",
        "- Go to **File ‚Üí Download ‚Üí Download .ipynb**\n",
        "- Save the file to your computer\n",
        "\n",
        "### Step 2: Upload to Colab\n",
        "- Click the **üìÅ folder icon** on the left sidebar\n",
        "- Click the **upload button**\n",
        "- Select your downloaded .ipynb file\n",
        "- Wait for the upload to complete\n",
        "\n",
        "### Step 3: Run the Code Below\n",
        "- **Uncomment the cell below** and run the cell\n",
        "- This will take about 1-2 minutes to install required packages\n",
        "\n",
        "### Step 4: Enter Notebook Name\n",
        "- When prompted, type your notebook name (e.g. `gs_000000_as4.ipynb`)\n",
        "- Press Enter\n",
        "\n",
        "### The PDF will automatically download to your computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf_code"
      },
      "outputs": [],
      "source": [
        "# # Install required packages (this takes about 30 seconds)\n",
        "# print(\"Installing PDF converter... please wait...\")\n",
        "# !apt-get update -qq\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic pandoc > /dev/null 2>&1\n",
        "# !pip install -q nbconvert\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"COLAB NOTEBOOK TO PDF CONVERTER\")\n",
        "# print(\"=\"*50)\n",
        "# print(\"\\nSTEP 1: Download your notebook\")\n",
        "# print(\"- Go to File ‚Üí Download ‚Üí Download .ipynb\")\n",
        "# print(\"- Save it to your computer\")\n",
        "# print(\"\\nSTEP 2: Upload it here\")\n",
        "# print(\"- Click the folder icon on the left (üìÅ)\")\n",
        "# print(\"- Click the upload button and select your .ipynb file\")\n",
        "# print(\"- Wait for upload to complete\")\n",
        "# print(\"\\nSTEP 3: Enter the filename below\")\n",
        "# print(\"=\"*50)\n",
        "\n",
        "# # Get notebook name from user\n",
        "# notebook_name = input(\"\\nEnter your notebook name: \")\n",
        "\n",
        "# # Add .ipynb if missing\n",
        "# if not notebook_name.endswith('.ipynb'):\n",
        "#     notebook_name += '.ipynb'\n",
        "\n",
        "# import os\n",
        "# notebook_path = f'/content/{notebook_name}'\n",
        "\n",
        "# # Check if file exists\n",
        "# if not os.path.exists(notebook_path):\n",
        "#     print(f\"\\n‚ö† Error: '{notebook_name}' not found in /content/\")\n",
        "#     print(\"\\nMake sure you uploaded the file using the folder icon (üìÅ) on the left!\")\n",
        "# else:\n",
        "#     print(f\"\\n‚úì Found {notebook_name}\")\n",
        "#     print(\"Converting to PDF... this may take 1-2 minutes...\\n\")\n",
        "\n",
        "#     # Convert the notebook to PDF\n",
        "#     !jupyter nbconvert --to pdf \"{notebook_path}\"\n",
        "\n",
        "#     # Download the PDF\n",
        "#     from google.colab import files\n",
        "#     pdf_name = notebook_name.replace('.ipynb', '.pdf')\n",
        "#     pdf_path = f'/content/{pdf_name}'\n",
        "\n",
        "#     if os.path.exists(pdf_path):\n",
        "#         print(\"‚úì SUCCESS! Downloading your PDF now...\")\n",
        "#         files.download(pdf_path)\n",
        "#         print(\"\\n‚úì Done! Check your downloads folder.\")\n",
        "#     else:\n",
        "#         print(\"‚ö† Error: Could not create PDF\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
