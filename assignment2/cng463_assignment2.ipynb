{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: N-grams and Language Identification\n",
    "## CNG463 - Introduction to Natural Language Processing\n",
    "### METU NCC Computer Engineering | Fall 2025-26\n",
    "\n",
    "**Student Name:**  \n",
    "**Student ID:**  \n",
    "**Due Date:** 16 November 2025 (Sunday) before midnight\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This assignment focuses on:\n",
    "1. Building 2-gram and 3-gram language models with Laplace smoothing\n",
    "2. Sentence-based language identification using 10-fold cross-validation\n",
    "3. Evaluation using accuracy, precision, recall, and F1-score\n",
    "4. Comparison and analysis\n",
    "\n",
    "**Grading:**\n",
    "- Written Questions (8 Ã— 3 pts): **24 pts**\n",
    "- Code Tasks with TODO (14 total): **76 pts** distributed by effort level:\n",
    "  - Simple tasks: 4 pts each (6 cells)\n",
    "  - Moderate tasks: 6 pts each (4 cells)\n",
    "  - Complex tasks: 7 pts each (4 cells)\n",
    "- **Total: 100 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "\n",
    "# Scikit-learn for cross-validation and metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# HuggingFace (will be installed when needed)\n",
    "# !pip install transformers -q\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 1: Corpus Preparation and Statistics (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Upload Corpus Files\n",
    "\n",
    "Prepare your text files in **two different languages** (accepted formats: `.txt`, `.pdf`, or `.docx`). When you run the cell below, you'll be prompted to upload files for each language separately. Make sure your files contain substantial text (reports, essays, or similar content from other courses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Upload your ENGLISH corpus file(s):\")\n",
    "english_files = files.upload()\n",
    "\n",
    "print(\"\\nUpload your SECOND LANGUAGE corpus file(s):\")\n",
    "second_lang_files = files.upload()\n",
    "\n",
    "\n",
    "# [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Load and Preprocess Data\n",
    "\n",
    "Load your uploaded files, extract text, preprocess, split into sentences, and tokenize. You'll need helper functions to handle different file formats.\n",
    "\n",
    "**Steps:**\n",
    "1. Read files based on format (`.txt`, `.pdf`, `.docx`) and combine into single text for each language\n",
    "2. Apply preprocessing (e.g., lowercasing, handling punctuation)\n",
    "3. Split each corpus into individual sentences\n",
    "4. Tokenize each sentence into a list of words\n",
    "5. Store the results as two lists of tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def read_txt_file(filename: str) -> str:\n",
    "    \"\"\"Read a .txt file and return its content.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf_file(filename: str) -> str:\n",
    "    \"\"\"Read a .pdf file and return its text content.\"\"\"\n",
    "    # TODO: Install and use PyPDF2 or pdfplumber\n",
    "    # Example: pip install PyPDF2\n",
    "    pass\n",
    "\n",
    "def read_docx_file(filename: str) -> str:\n",
    "    \"\"\"Read a .docx file and return its text content.\"\"\"\n",
    "    # TODO: Install and use python-docx\n",
    "    # Example: pip install python-docx\n",
    "    pass\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    # TODO: Implement sentence splitting\n",
    "    # You can use simple regex or nltk.sent_tokenize\n",
    "    pass\n",
    "\n",
    "def tokenize_sentence(sentence: str) -> List[str]:\n",
    "    \"\"\"Tokenize a sentence into words.\"\"\"\n",
    "    # TODO: Implement word tokenization\n",
    "    # You can use str.split() or nltk.word_tokenize\n",
    "    pass\n",
    "\n",
    "# TODO: Your code here\n",
    "# \n",
    "# 1. Read and combine files for each language\n",
    "#    - Loop through lang1_files and lang2_files\n",
    "#    - Use appropriate read function based on file extension\n",
    "#    - Combine all text into lang1_text and lang2_text\n",
    "#\n",
    "# 2. Apply preprocessing to both lang1_text and lang2_text\n",
    "#    (e.g., lowercasing, removing extra whitespace)\n",
    "#\n",
    "# 3. Split each corpus into sentences using split_into_sentences()\n",
    "#\n",
    "# 4. Tokenize each sentence using tokenize_sentence()\n",
    "#\n",
    "# 5. Store results as:\n",
    "#    - lang1_sentences: List[List[str]] (list of tokenized sentences)\n",
    "#    - lang2_sentences: List[List[str]] (list of tokenized sentences)\n",
    "\n",
    "lang1_text = \"\"\n",
    "lang2_text = \"\"\n",
    "\n",
    "# Your code continues...\n",
    "\n",
    "# At the end, you should have:\n",
    "# lang1_sentences = [[word1, word2, ...], [word1, word2, ...], ...]\n",
    "# lang2_sentences = [[word1, word2, ...], [word1, word2, ...], ...]\n",
    "\n",
    "# [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1:** What preprocessing choices did you make and why? (3-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Basic Statistics\n",
    "\n",
    "Calculate and display key statistics for both language corpora to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate statistics for BOTH languages\n",
    "#\n",
    "# For each language (lang1_sentences and lang2_sentences):\n",
    "# - Total word count\n",
    "# - Vocabulary size (unique words)\n",
    "# - Sentence count\n",
    "# - Average sentence length\n",
    "# - Special character/punctuation frequency\n",
    "#\n",
    "# Example structure:\n",
    "# lang1_total_words = sum(len(sentence) for sentence in lang1_sentences)\n",
    "# lang1_vocabulary = len(set(word for sentence in lang1_sentences for word in sentence))\n",
    "# ...\n",
    "#\n",
    "# Create a comparison table or print statistics side by side\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2:** What are the key differences between your two corpora? (2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 2: N-gram Language Identification (35 points)\n",
    "\n",
    "**Baseline (25 pts):** Implement 2-gram and 3-gram models, run 10-fold CV, report accuracy.  \n",
    "**Creativity Bonus (10 pts):** Out-of-vocabulary analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Implement N-gram Models\n",
    "\n",
    "Implement the `NgramLanguageModel` class with Laplace smoothing using NLTK's n-gram utilities. The model should count n-grams during training and calculate sentence probabilities with smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams, pad_sequence\n",
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from typing import List\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    \"\"\"\n",
    "    N-gram language model with Laplace (add-1) smoothing using NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize the n-gram model.\n",
    "        \n",
    "        Args:\n",
    "            n: Order of n-gram (2 for bigram, 3 for trigram)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.model = Laplace(n)\n",
    "    \n",
    "    def train(self, sentences: List[List[str]]):\n",
    "        \"\"\"\n",
    "        Train the model on a list of tokenized sentences.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of tokenized sentences (each sentence is a list of words)\n",
    "        \"\"\"\n",
    "        # TODO: Your code here\n",
    "        # Use padded_everygram_pipeline to prepare training data with padding\n",
    "        # Then fit the model using self.model.fit()\n",
    "        pass\n",
    "    \n",
    "    def get_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the probability of a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: Tokenized sentence\n",
    "            \n",
    "        Returns:\n",
    "            Probability of the sentence\n",
    "        \"\"\"\n",
    "        # TODO: Your code here\n",
    "        # Pad the sentence and generate n-grams\n",
    "        # For each n-gram, get probability using self.model.score()\n",
    "        # Multiply probabilities together (or sum log probabilities to avoid underflow)\n",
    "        pass\n",
    "\n",
    "# [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot Check: Inspect Your N-gram Models\n",
    "\n",
    "After implementing the model, train sample models on both languages and inspect what they learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train sample models and inspect them\n",
    "#\n",
    "# 1. Create 2-gram and 3-gram models for both languages\n",
    "# 2. Train them on your full datasets (lang1_sentences and lang2_sentences)\n",
    "# 3. Inspect the models to see what n-grams they learned\n",
    "#\n",
    "# Example:\n",
    "# model_2gram_lang1 = NgramLanguageModel(n=2)\n",
    "# model_2gram_lang1.train(lang1_sentences)\n",
    "# model_3gram_lang1 = NgramLanguageModel(n=3)\n",
    "# model_3gram_lang1.train(lang1_sentences)\n",
    "#\n",
    "# model_2gram_lang2 = NgramLanguageModel(n=2)\n",
    "# model_2gram_lang2.train(lang2_sentences)\n",
    "# model_3gram_lang2 = NgramLanguageModel(n=3)\n",
    "# model_3gram_lang2.train(lang3_sentences)\n",
    "#\n",
    "# Display sample n-grams and their counts from each model\n",
    "# Check vocabulary size: len(model.model.vocab)\n",
    "# Show most common n-grams or test probabilities on sample sentences\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Implement Language Identification\n",
    "\n",
    "Create a function that compares sentence probabilities from two language models and returns the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_language(sentence: List[str], \n",
    "                     model_lang1: NgramLanguageModel, \n",
    "                     model_lang2: NgramLanguageModel) -> int:\n",
    "    \"\"\"\n",
    "    Identify the language of a sentence using two language models.\n",
    "    \n",
    "    Args:\n",
    "        sentence: Tokenized sentence\n",
    "        model_lang1: Language model for language 1 (label 0)\n",
    "        model_lang2: Language model for language 2 (label 1)\n",
    "        \n",
    "    Returns:\n",
    "        Predicted label (0 or 1)\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    # \n",
    "    # Steps:\n",
    "    # 1. Calculate probability of sentence using model_lang1\n",
    "    #    prob1 = model_lang1.get_probability(sentence)\n",
    "    #\n",
    "    # 2. Calculate probability of sentence using model_lang2\n",
    "    #    prob2 = model_lang2.get_probability(sentence)\n",
    "    #\n",
    "    # 3. Compare probabilities and return the label of the model with higher probability\n",
    "    #    if prob1 > prob2: return 0\n",
    "    #    else: return 1\n",
    "    \n",
    "    pass\n",
    "\n",
    "# [6 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Implement Evaluation Function\n",
    "\n",
    "Create a function that calculates accuracy, precision, recall, and F1-score given predicted and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with accuracy, precision, recall, f1_score\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    # Use sklearn's accuracy_score and precision_recall_fscore_support\n",
    "    pass\n",
    "\n",
    "# [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: Prepare Dataset for Cross-Validation\n",
    "\n",
    "Create `X` (all sentences) and `y` (labels) for CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "#\n",
    "# Create:\n",
    "# X = list of all tokenized sentences from both languages\n",
    "# y = list of labels (0 for language 1, 1 for language 2)\n",
    "#\n",
    "# Example:\n",
    "# X = english_sentences + second_lang_sentences\n",
    "# y = [0] * len(english_sentences) + [1] * len(second_lang_sentences)\n",
    "\n",
    "# [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: 10-Fold Cross-Validation for Language Identification\n",
    "\n",
    "**Overview:** We will compare 2-gram and 3-gram models for sentence-based language identification. In each fold, we split sentences into training and test sets. For each n-gram order (2 and 3), we train two modelsâ€”one per languageâ€”on the training sentences. Then, we use `identify_language()` to classify test sentences by comparing their probabilities under both language models. Finally, we evaluate performance using `calculate_metrics()` and repeat this process across all 10 folds to get robust performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Prepare dataset: combine sentences from both languages with labels\n",
    "X = lang1_sentences + lang2_sentences\n",
    "y = [0] * len(lang1_sentences) + [1] * len(lang2_sentences)\n",
    "\n",
    "print(f\"Dataset prepared:\")\n",
    "print(f\"  Total sentences: {len(X)}\")\n",
    "print(f\"  Language 1 (label 0): {sum(1 for label in y if label == 0)} sentences\")\n",
    "print(f\"  Language 2 (label 1): {sum(1 for label in y if label == 1)} sentences\")\n",
    "print()\n",
    "\n",
    "# Initialize 10-fold cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold\n",
    "results_2gram = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "results_3gram = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# For each fold:\n",
    "#   1. Split data: X_train, X_test, y_train, y_test using train_idx and test_idx\n",
    "#   2. Separate training sentences by language:\n",
    "#      - train_lang1 = sentences where y_train == 0\n",
    "#      - train_lang2 = sentences where y_train == 1\n",
    "#   3. Train four models:\n",
    "#      - model_2gram_lang1 and model_2gram_lang2 (both with n=2)\n",
    "#      - model_3gram_lang1 and model_3gram_lang2 (both with n=3)\n",
    "#   4. Make predictions on X_test:\n",
    "#      - pred_2gram = [identify_language(sent, model_2gram_lang1, model_2gram_lang2) for sent in X_test]\n",
    "#      - pred_3gram = [identify_language(sent, model_3gram_lang1, model_3gram_lang2) for sent in X_test]\n",
    "#   5. Calculate metrics:\n",
    "#      - metrics_2gram = calculate_metrics(y_test, pred_2gram)\n",
    "#      - metrics_3gram = calculate_metrics(y_test, pred_3gram)\n",
    "#   6. Store results:\n",
    "#      - results_2gram['accuracy'].append(metrics_2gram['accuracy'])\n",
    "#      - (repeat for precision, recall, f1 for both models)\n",
    "\n",
    "# TODO: Complete the cross-validation loop\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X), 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx}/10\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get train and test data for this fold\n",
    "    X_train = [X[i] for i in train_idx]\n",
    "    y_train = [y[i] for i in train_idx]\n",
    "    X_test = [X[i] for i in test_idx]\n",
    "    y_test = [y[i] for i in test_idx]\n",
    "    \n",
    "    # Your implementation here\n",
    "    \n",
    "    # Print fold results (after you calculate them)\n",
    "    # print(f\"Fold {fold_idx} - 2-gram accuracy: {results_2gram['accuracy'][-1]:.4f}\")\n",
    "    # print(f\"Fold {fold_idx} - 3-gram accuracy: {results_3gram['accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cross-validation completed!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5: Display Results\n",
    "\n",
    "*Create a table showing for each model:*\n",
    "Mean accuracy, precision, recall, F1 (with std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and display summary statistics\n",
    "#\n",
    "#\n",
    "# Example:\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Model': ['2-gram', '3-gram'],\n",
    "#     'Accuracy': [...],\n",
    "#     'Precision': [...],\n",
    "#     ...\n",
    "# })\n",
    "\n",
    "# [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1:** Which of your trained models performed best on the validation data, and why? (3-4 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2:** Were the results consistent across different folds of cross-validation? (2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6: Out-of-Vocabulary Testing (Bonus: 10 pts)\n",
    "\n",
    "Test your models with **five** sentences containing words not in your training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and test OOV sentences\n",
    "#\n",
    "# Ideas:\n",
    "# - Sentences with made-up words\n",
    "# - Mix of real and invented words  \n",
    "# - Be creative!\n",
    "\n",
    "# [7 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3:** How well did your models handle out-of-vocabulary (OOV) samples? Explain briefly. (2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 3: Statistical Analysis and Visualization (35 points)\n",
    "\n",
    "**Baseline (25 pts):** Statistical significance testing and comparison visualization.  \n",
    "**Creativity Bonus (10 pts):** Advanced analysis (confusion matrices, error analysis, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Statistical Significance Testing\n",
    "\n",
    "Use paired t-test to compare models. p-value < 0.05 indicates statistically significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform paired t-tests\n",
    "#\n",
    "# Compare: 2-gram vs 3-gram\n",
    "#\n",
    "# Use: t_stat, p_value = ttest_rel(results_1, results_2)\n",
    "\n",
    "# [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** Are the performance differences statistically significant? Explain what 'statistical significance' means in this context. (2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualizations\n",
    "#\n",
    "# Suggestions:\n",
    "# 1. Box plot: accuracy distribution across folds for all models\n",
    "# 2. Bar plot: mean metrics (accuracy, precision, recall, F1) with error bars\n",
    "\n",
    "# [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2:** Based on all reported metrics, which model would you choose for a real-world application, and why? (3-4 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Advanced Analysis (Bonus: 10 pts)\n",
    "\n",
    "Perform deeper analysis such as per-language performance, misclassification patterns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your advanced analysis here\n",
    "\n",
    "# [4 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** What interesting patterns or insights did you discover from your results? (4-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[YOUR ANSWER HERE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pre-Submission Checklist\n",
    "\n",
    "- [ ] Name and student ID at top\n",
    "- [ ] All TODO sections completed\n",
    "- [ ] All questions answered\n",
    "- [ ] Code runs without errors\n",
    "- [ ] Results tables and visualizations included\n",
    "- [ ] Cross-validation implemented correctly\n",
    "- [ ] All three models compared (2-gram, 3-gram, HuggingFace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
