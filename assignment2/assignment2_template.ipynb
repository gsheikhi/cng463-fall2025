{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: N-grams and Language Identification\n",
    "## CNG463 - Introduction to Natural Language Processing\n",
    "### METU NCC Computer Engineering\n",
    "### Fall 2025-26\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Information\n",
    "\n",
    "**Due Date:** 15/11/2025 23:59\n",
    "\n",
    "**Submission:** Submit through ODTUClass:\n",
    "1. PDF export of this notebook\n",
    "2. This .ipynb file (backup)\n",
    "\n",
    "**File Naming:** `FirstnameLastname_StudentID_as2.pdf` and `.ipynb`\n",
    "\n",
    "**Critical:**\n",
    "- Email submissions will be deleted immediately and receive 0 points\n",
    "- Late submissions are not permitted\n",
    "\n",
    "---\n",
    "\n",
    "## Student Information\n",
    "\n",
    "**Full Name:** [YOUR FULL NAME HERE]\n",
    "\n",
    "**Student ID:** [YOUR STUDENT ID HERE]\n",
    "\n",
    "**Submission Date:** [DATE]\n",
    "\n",
    "**Languages Used:** English + [YOUR SECOND LANGUAGE]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This assignment provides hands-on experience with:\n",
    "1. **N-gram language models** (2-grams and 3-grams)\n",
    "2. **Language identification** using statistical methods\n",
    "3. **Text generation** with n-gram models\n",
    "4. **Evaluation metrics** (accuracy, precision, recall, F1-score)\n",
    "5. **Cross-validation** and **statistical significance testing**\n",
    "\n",
    "### Your Corpus\n",
    "\n",
    "You will use **your own written reports** from other courses as the corpus:\n",
    "- If you have reports in two languages ‚Üí use them directly\n",
    "- If you only have English reports ‚Üí translate them to a second language using AI tools\n",
    "- **Minimum corpus size:** ~5,000 words per language (combine multiple reports if needed)\n",
    "\n",
    "### Grading Summary\n",
    "\n",
    "**Total: 100 points**\n",
    "- Task 1 (Corpus Statistics): 15 pts (10 baseline + 5 creativity)\n",
    "- Task 2 (N-gram Language ID): 30 pts (20 baseline + 10 creativity)\n",
    "- Task 3 (Evaluation & Comparison): 20 pts (15 baseline + 5 creativity)\n",
    "- Task 4 (Text Generation): 20 pts (15 baseline + 5 creativity)\n",
    "- Task 5 (Reflection): 15 pts (10 baseline + 5 creativity)\n",
    "\n",
    "**Note:** Completing only baseline requirements yields 70/100. The remaining 30 points reward creativity, deeper analysis, and original insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Using This Notebook\n",
    "\n",
    "### Getting Started\n",
    "1. **Make your own copy:** `File ‚Üí Save a copy in Drive`\n",
    "2. **Rename it:** Include your name and student ID\n",
    "3. **Work in order:** Complete tasks sequentially\n",
    "4. **Save frequently:** Colab auto-saves, but be safe!\n",
    "\n",
    "### Where to Write Code\n",
    "- Look for cells marked with `# TODO: Your code here`\n",
    "- You can add more code cells as needed\n",
    "- Keep code clean and well-commented\n",
    "\n",
    "### Where to Write Analysis\n",
    "- Look for markdown sections: **[YOUR ANALYSIS HERE]**\n",
    "- Double-click to edit markdown cells\n",
    "- Write clear, concise observations\n",
    "\n",
    "### Before Submitting\n",
    "1. **Fill in** your name and student ID at the top\n",
    "2. **Run all cells:** `Runtime ‚Üí Run all`\n",
    "3. **Check outputs:** Ensure all cells executed without errors\n",
    "4. **Export to PDF:** `File ‚Üí Print ‚Üí Save as PDF`\n",
    "5. **Download .ipynb:** `File ‚Üí Download ‚Üí Download .ipynb`\n",
    "6. **Submit both files** to ODTUClass\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Run this cell first to import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# For HuggingFace model\n",
    "!pip install -q transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 1: Corpus Preparation and Statistics (15 points)\n",
    "\n",
    "**Baseline (10 pts):** Upload and analyze your corpus in two languages. Calculate basic statistics including word count, vocabulary size, sentence count, special characters, and other relevant metrics.\n",
    "\n",
    "**Creativity Bonus (5 pts):** Provide additional interesting statistical analyses (e.g., word length distribution, most frequent words, character n-gram analysis, type-token ratio, etc.) with visualization and insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Upload Your Corpus Files\n",
    "\n",
    "Upload your text files (reports from other courses) in two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload English corpus\n",
    "print(\"üìÅ Upload your ENGLISH corpus file(s):\")\n",
    "print(\"(You can select multiple files if needed)\\n\")\n",
    "uploaded_en = files.upload()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Upload second language corpus\n",
    "print(\"üìÅ Upload your SECOND LANGUAGE corpus file(s):\")\n",
    "print(\"(You can select multiple files if needed)\\n\")\n",
    "uploaded_lang2 = files.upload()\n",
    "\n",
    "print(\"\\n‚úì Files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Load and Preprocess Corpora\n",
    "\n",
    "Load your corpus files and perform basic preprocessing:\n",
    "- Read all files and combine them\n",
    "- Decide on preprocessing steps (lowercasing, punctuation, etc.)\n",
    "- Tokenize into sentences and words\n",
    "\n",
    "**Note:** Document your preprocessing decisions - they affect your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "# \n",
    "# Suggested steps:\n",
    "# 1. Read uploaded files\n",
    "# 2. Combine multiple files if needed\n",
    "# 3. Basic cleaning (optional: remove extra whitespace, etc.)\n",
    "# 4. Sentence tokenization\n",
    "# 5. Word tokenization\n",
    "#\n",
    "# Example structure:\n",
    "# english_text = \"\"  # Combined text from all English files\n",
    "# lang2_text = \"\"     # Combined text from all second language files\n",
    "# \n",
    "# english_sentences = [...]  # List of sentences\n",
    "# lang2_sentences = [...]\n",
    "#\n",
    "# english_words = [...]  # List of all words\n",
    "# lang2_words = [...]\n",
    "\n",
    "# Hint: You might want to use simple splits or regex for tokenization\n",
    "# Example: sentences = text.split('.') or re.split(r'[.!?]+', text)\n",
    "# Example: words = sentence.lower().split() or re.findall(r'\\b\\w+\\b', sentence)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Decisions\n",
    "\n",
    "**[YOUR ANALYSIS HERE]**\n",
    "\n",
    "Document the preprocessing choices you made:\n",
    "- Did you lowercase the text? Why or why not?\n",
    "- How did you handle punctuation?\n",
    "- What method did you use for sentence tokenization?\n",
    "- What method did you use for word tokenization?\n",
    "- Any other preprocessing steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Basic Statistical Analysis\n",
    "\n",
    "Calculate and display key statistics for both corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "#\n",
    "# Required statistics (baseline):\n",
    "# 1. Total word count\n",
    "# 2. Vocabulary size (unique words)\n",
    "# 3. Number of sentences\n",
    "# 4. Average sentence length (in words)\n",
    "# 5. Special characters count (e.g., Turkish: √ß, ƒü, ƒ±, √∂, ≈ü, √º)\n",
    "# 6. Any other relevant statistics\n",
    "#\n",
    "# Hint: Use Counter, set(), len() functions\n",
    "# Hint: Create a nice comparison table or visualization\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4: Additional Statistical Analysis (Creativity Bonus)\n",
    "\n",
    "Perform additional interesting analyses. Examples:\n",
    "- Word length distribution\n",
    "- Most frequent words (top 20)\n",
    "- Type-token ratio (vocabulary richness)\n",
    "- Character n-gram frequencies\n",
    "- Sentence length distribution\n",
    "- Zipf's law visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your creative analysis here\n",
    "# \n",
    "# Suggestions:\n",
    "# - Create visualizations (histograms, bar charts, word clouds)\n",
    "# - Compare the two languages\n",
    "# - Look for interesting patterns\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis: Observations and Insights\n",
    "\n",
    "**[YOUR ANALYSIS HERE]**\n",
    "\n",
    "Discuss your findings:\n",
    "- What are the key differences between the two corpora?\n",
    "- Are there significant differences in vocabulary size relative to corpus size?\n",
    "- What interesting patterns did you observe?\n",
    "- How might these statistics affect n-gram modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 2: N-gram Language Identification (30 points)\n",
    "\n",
    "Build 2-gram and 3-gram language models using **10-fold cross-validation** and evaluate language identification performance.\n",
    "\n",
    "**Baseline (20 pts):** \n",
    "- Implement 2-gram and 3-gram models with Laplace smoothing\n",
    "- Use 10-fold cross-validation\n",
    "- Calculate accuracy for both models across all folds\n",
    "- Report mean and standard deviation of accuracy\n",
    "\n",
    "**Creativity Bonus (10 pts):**\n",
    "- Error analysis: which sentences are misclassified and why?\n",
    "- Compare 2-gram vs 3-gram performance\n",
    "- Test with different smoothing parameters\n",
    "- Analyze confidence scores\n",
    "- Any other creative exploration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: N-gram Model Implementation\n",
    "\n",
    "Implement n-gram language models with Laplace (add-1) smoothing.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **N-gram:** Sequence of n words (2-gram: \"the cat\", 3-gram: \"the black cat\")\n",
    "- **Language Model:** Probability distribution over word sequences\n",
    "- **Laplace Smoothing:** Add 1 to all counts to handle unseen n-grams\n",
    "- **Formula:** P(word|context) = (count(context, word) + 1) / (count(context) + V)\n",
    "  - V = vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    \"\"\"\n",
    "    N-gram language model with Laplace smoothing.\n",
    "    \n",
    "    This model learns the probability distribution of word sequences\n",
    "    and can be used for language identification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 2):\n",
    "        \"\"\"\n",
    "        Initialize the n-gram model.\n",
    "        \n",
    "        Args:\n",
    "            n: The n in n-gram (2 for bigram, 3 for trigram)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)  # Count of each n-gram\n",
    "        self.context_counts = defaultdict(int)  # Count of each context (n-1 words)\n",
    "        self.vocabulary = set()  # All unique words seen\n",
    "        \n",
    "    def train(self, sentences: List[List[str]]):\n",
    "        \"\"\"\n",
    "        Train the model on a list of tokenized sentences.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences, where each sentence is a list of words\n",
    "        \"\"\"\n",
    "        # TODO: Implement training\n",
    "        # \n",
    "        # Steps:\n",
    "        # 1. Add start/end tokens to sentences (e.g., <s>, </s>)\n",
    "        # 2. Extract all n-grams from sentences\n",
    "        # 3. Count n-grams and contexts\n",
    "        # 4. Build vocabulary\n",
    "        #\n",
    "        # Hint: Use tuple for n-grams, e.g., ('the', 'cat') for bigram\n",
    "        # Hint: Context is the first (n-1) words of the n-gram\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_probability(self, ngram: Tuple[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the probability of an n-gram using Laplace smoothing.\n",
    "        \n",
    "        Args:\n",
    "            ngram: Tuple of n words\n",
    "            \n",
    "        Returns:\n",
    "            Smoothed probability of the n-gram\n",
    "        \"\"\"\n",
    "        # TODO: Implement Laplace smoothing\n",
    "        #\n",
    "        # Formula: P(word|context) = (count(ngram) + 1) / (count(context) + V)\n",
    "        # where V is the vocabulary size\n",
    "        #\n",
    "        # Steps:\n",
    "        # 1. Extract context (first n-1 words)\n",
    "        # 2. Get counts\n",
    "        # 3. Apply smoothing formula\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log probability of a sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: List of words\n",
    "            \n",
    "        Returns:\n",
    "            Log probability of the sentence\n",
    "        \"\"\"\n",
    "        # TODO: Implement sentence scoring\n",
    "        #\n",
    "        # Steps:\n",
    "        # 1. Add start/end tokens\n",
    "        # 2. Extract all n-grams\n",
    "        # 3. Sum log probabilities (use np.log to avoid underflow)\n",
    "        #\n",
    "        # Hint: log(P1 * P2 * P3) = log(P1) + log(P2) + log(P3)\n",
    "        \n",
    "        pass\n",
    "\n",
    "print(\"‚úì NgramLanguageModel class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Language Identification Function\n",
    "\n",
    "Create a function that uses two language models to identify the language of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_language(sentence: List[str], \n",
    "                     model_lang1: NgramLanguageModel, \n",
    "                     model_lang2: NgramLanguageModel,\n",
    "                     lang1_name: str = \"English\",\n",
    "                     lang2_name: str = \"Language2\") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Identify which language a sentence belongs to.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of words to classify\n",
    "        model_lang1: Trained n-gram model for language 1\n",
    "        model_lang2: Trained n-gram model for language 2\n",
    "        lang1_name: Name of language 1\n",
    "        lang2_name: Name of language 2\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predicted_language, confidence_score)\n",
    "    \"\"\"\n",
    "    # TODO: Implement language identification\n",
    "    #\n",
    "    # Steps:\n",
    "    # 1. Calculate log probability under each model\n",
    "    # 2. Choose language with higher log probability\n",
    "    # 3. Calculate confidence (optional: difference in log probs)\n",
    "    \n",
    "    pass\n",
    "\n",
    "print(\"‚úì identify_language function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Prepare Data for Cross-Validation\n",
    "\n",
    "Organize your data for 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare your data\n",
    "#\n",
    "# Create a dataset where:\n",
    "# - X = list of all sentences (from both languages)\n",
    "# - y = list of labels (0 for English, 1 for second language)\n",
    "#\n",
    "# Example structure:\n",
    "# X = english_sentences + lang2_sentences\n",
    "# y = [0] * len(english_sentences) + [1] * len(lang2_sentences)\n",
    "#\n",
    "# Shuffle the data (optional but recommended)\n",
    "\n",
    "# X = []  # All sentences\n",
    "# y = []  # Corresponding labels\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: 10-Fold Cross-Validation\n",
    "\n",
    "Implement 10-fold cross-validation for both 2-gram and 3-gram models.\n",
    "\n",
    "**What is Cross-Validation?**\n",
    "- Split data into 10 equal parts (folds)\n",
    "- Train on 9 folds, test on 1 fold\n",
    "- Repeat 10 times (each fold serves as test set once)\n",
    "- Average results across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement 10-fold cross-validation\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create KFold object with 10 splits\n",
    "# 2. For each fold:\n",
    "#    a. Split data into train and test\n",
    "#    b. Train 2-gram models on both languages\n",
    "#    c. Train 3-gram models on both languages\n",
    "#    d. Test both models on test set\n",
    "#    e. Calculate accuracy for both\n",
    "# 3. Store results for each fold\n",
    "#\n",
    "# Hint: Use sklearn.model_selection.KFold\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results_2gram = []  # Accuracy for each fold (2-gram)\n",
    "results_3gram = []  # Accuracy for each fold (3-gram)\n",
    "\n",
    "# TODO: Implement the cross-validation loop\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5: Results Summary\n",
    "\n",
    "Display cross-validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and display results\n",
    "#\n",
    "# For both 2-gram and 3-gram:\n",
    "# - Mean accuracy across folds\n",
    "# - Standard deviation\n",
    "# - Create a visualization (e.g., bar plot comparing models)\n",
    "# - Create a table showing fold-by-fold results\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6: Error Analysis (Creativity Bonus)\n",
    "\n",
    "Analyze misclassified sentences to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Error analysis (for creativity bonus)\n",
    "#\n",
    "# Suggestions:\n",
    "# - Collect all misclassified sentences from one fold\n",
    "# - Analyze their characteristics (length, vocabulary, etc.)\n",
    "# - Show examples of misclassified sentences\n",
    "# - Discuss why they might be difficult\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Identification: Analysis and Observations\n",
    "\n",
    "**[YOUR ANALYSIS HERE]**\n",
    "\n",
    "Discuss your findings:\n",
    "- How did 2-gram and 3-gram models compare?\n",
    "- What accuracy did you achieve?\n",
    "- Were the results consistent across folds?\n",
    "- What types of sentences were most difficult to classify?\n",
    "- Did you notice any patterns in the errors?\n",
    "- How did model performance relate to your corpus statistics from Task 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 3: Evaluation and Comparison (20 points)\n",
    "\n",
    "Calculate comprehensive evaluation metrics and compare with a HuggingFace model.\n",
    "\n",
    "**Baseline (15 pts):**\n",
    "- Calculate precision, recall, and F1-score for your models\n",
    "- Compare your models with a HuggingFace language identification model\n",
    "- Perform paired t-test for statistical significance\n",
    "- Report and interpret results\n",
    "\n",
    "**Creativity Bonus (5 pts):**\n",
    "- Confusion matrices\n",
    "- Per-class metrics analysis\n",
    "- Discussion of when each model performs better\n",
    "- Additional statistical analyses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Detailed Evaluation Metrics\n",
    "\n",
    "Calculate precision, recall, and F1-score for your n-gram models.\n",
    "\n",
    "**Metrics Explained:**\n",
    "- **Accuracy:** Overall correctness (TP + TN) / Total\n",
    "- **Precision:** Of predicted positives, how many are correct? TP / (TP + FP)\n",
    "- **Recall:** Of actual positives, how many did we find? TP / (TP + FN)\n",
    "- **F1-Score:** Harmonic mean of precision and recall: 2 * (P * R) / (P + R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-run cross-validation and collect predictions\n",
    "#\n",
    "# This time, store predictions for each fold to calculate all metrics\n",
    "#\n",
    "# For each fold, store:\n",
    "# - y_true: actual labels\n",
    "# - y_pred_2gram: predictions from 2-gram model\n",
    "# - y_pred_3gram: predictions from 3-gram model\n",
    "\n",
    "# Initialize storage for all folds\n",
    "all_metrics_2gram = []\n",
    "all_metrics_3gram = []\n",
    "\n",
    "# TODO: Run cross-validation and collect metrics\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate and display comprehensive metrics\n",
    "#\n",
    "# For both models, calculate:\n",
    "# - Mean and std of accuracy, precision, recall, F1\n",
    "# - Create a comparison table\n",
    "# - Visualize metrics\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: HuggingFace Model Comparison\n",
    "\n",
    "Compare your models with a pre-trained language identification model.\n",
    "\n",
    "**Recommended Model:** `papluca/xlm-roberta-base-language-detection` (lightweight, 218 languages)\n",
    "\n",
    "**Note:** This model is trained on many languages and may not be specifically optimized for your language pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace model\n",
    "print(\"Loading HuggingFace language identification model...\")\n",
    "print(\"This may take a minute on first run.\\n\")\n",
    "\n",
    "hf_model = pipeline(\"text-classification\", \n",
    "                   model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "                   device=-1)  # Use CPU (-1) or GPU (0)\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate HuggingFace model on the same folds\n",
    "#\n",
    "# Steps:\n",
    "# 1. For each fold in your cross-validation:\n",
    "#    a. Get predictions from HuggingFace model on test set\n",
    "#    b. Calculate accuracy (and other metrics)\n",
    "# 2. Store results for comparison\n",
    "#\n",
    "# Hint: hf_model(sentence_string) returns prediction\n",
    "# Hint: Map HuggingFace language codes to your labels\n",
    "#       e.g., 'en' -> 0, 'tr' -> 1\n",
    "\n",
    "results_hf = []  # Accuracy for each fold (HuggingFace)\n",
    "\n",
    "# TODO: Implement HuggingFace evaluation\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Statistical Significance Testing\n",
    "\n",
    "Use paired t-test to determine if differences between models are statistically significant.\n",
    "\n",
    "**Why Paired T-Test?**\n",
    "- Same data (same folds) for all models\n",
    "- Tests if mean difference is significantly different from zero\n",
    "- Null hypothesis (H0): No difference between models\n",
    "- Alternative hypothesis (H1): There is a difference\n",
    "\n",
    "**Interpretation:**\n",
    "- p-value < 0.05: Reject H0, difference is statistically significant\n",
    "- p-value ‚â• 0.05: Fail to reject H0, difference could be due to chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform paired t-tests\n",
    "#\n",
    "# Compare:\n",
    "# 1. 2-gram vs 3-gram\n",
    "# 2. 2-gram vs HuggingFace\n",
    "# 3. 3-gram vs HuggingFace\n",
    "#\n",
    "# Use: stats.ttest_rel(results1, results2)\n",
    "\n",
    "# Example:\n",
    "# t_stat, p_value = stats.ttest_rel(results_2gram, results_3gram)\n",
    "# print(f\"2-gram vs 3-gram: t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4: Comprehensive Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualizations comparing all models\n",
    "#\n",
    "# Suggestions:\n",
    "# - Box plots showing distribution of accuracies across folds\n",
    "# - Bar chart comparing mean accuracies with error bars\n",
    "# - Line plot showing fold-by-fold performance\n",
    "# - Table with all metrics (mean ¬± std)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5: Additional Analysis (Creativity Bonus)\n",
    "\n",
    "Deeper analysis of model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Additional analysis (for creativity bonus)\n",
    "#\n",
    "# Ideas:\n",
    "# - Confusion matrices for each model\n",
    "# - Per-language precision/recall comparison\n",
    "# - Analysis of sentences where models disagree\n",
    "# - Correlation analysis between models\n",
    "# - Performance vs sentence length\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison: Analysis and Observations\n",
    "\n",
    "**[YOUR ANALYSIS HERE]**\n",
    "\n",
    "Discuss your findings:\n",
    "- How did your n-gram models compare to HuggingFace?\n",
    "- Were the differences statistically significant? What does this mean?\n",
    "- Which model performed best? Why do you think so?\n",
    "- What are the advantages and disadvantages of each approach?\n",
    "- In what scenarios might your simple n-gram model be preferable to a complex neural model?\n",
    "- How did the evaluation metrics (precision, recall, F1) provide additional insights beyond accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 4: Text Generation with N-grams (20 points)\n",
    "\n",
    "Use your English n-gram models to generate sample sentences.\n",
    "\n",
    "**Baseline (15 pts):**\n",
    "- Generate 10 sentences using your 2-gram English model\n",
    "- Generate 10 sentences using your 3-gram English model\n",
    "- Compare quality and coherence\n",
    "- Discuss observations\n",
    "\n",
    "**Creativity Bonus (5 pts):**\n",
    "- Experiment with different generation strategies (e.g., temperature, top-k sampling)\n",
    "- Compare sentences with different starting words\n",
    "- Analyze common patterns or errors\n",
    "- Generate sentences in your second language and compare\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Text Generation Implementation\n",
    "\n",
    "Implement sentence generation using n-gram models.\n",
    "\n",
    "**How N-gram Generation Works:**\n",
    "1. Start with initial context (e.g., `<s>` token or a seed word)\n",
    "2. Look at all n-grams starting with current context\n",
    "3. Sample next word based on n-gram probabilities\n",
    "4. Update context (shift by one word)\n",
    "5. Repeat until end token or max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model: NgramLanguageModel, \n",
    "                     max_length: int = 20,\n",
    "                     start_word: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a sentence using an n-gram model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained n-gram language model\n",
    "        max_length: Maximum number of words to generate\n",
    "        start_word: Optional starting word (if None, use <s>)\n",
    "        \n",
    "    Returns:\n",
    "        Generated sentence as string\n",
    "    \"\"\"\n",
    "    # TODO: Implement text generation\n",
    "    #\n",
    "    # Steps:\n",
    "    # 1. Initialize context (start tokens or seed word)\n",
    "    # 2. Loop until </s> or max_length:\n",
    "    #    a. Find all possible next words given current context\n",
    "    #    b. Get probability for each possible next word\n",
    "    #    c. Sample next word (use np.random.choice with probabilities)\n",
    "    #    d. Add word to sentence\n",
    "    #    e. Update context (shift window)\n",
    "    # 3. Return generated sentence\n",
    "    #\n",
    "    # Hint: You may need to add methods to NgramLanguageModel class\n",
    "    #       to get possible next words and their probabilities\n",
    "    \n",
    "    pass\n",
    "\n",
    "print(\"‚úì generate_sentence function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: Train Models for Generation\n",
    "\n",
    "Train 2-gram and 3-gram models on your complete English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train models on full English corpus (not cross-validation)\n",
    "#\n",
    "# Train:\n",
    "# - english_model_2gram: 2-gram model\n",
    "# - english_model_3gram: 3-gram model\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Generate Sample Sentences\n",
    "\n",
    "Generate 10 sentences with each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with 2-gram model\n",
    "print(\"=\" * 60)\n",
    "print(\"2-GRAM GENERATED SENTENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Generate 10 sentences\n",
    "# for i in range(10):\n",
    "#     sentence = generate_sentence(english_model_2gram)\n",
    "#     print(f\"{i+1}. {sentence}\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with 3-gram model\n",
    "print(\"=\" * 60)\n",
    "print(\"3-GRAM GENERATED SENTENCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Generate 10 sentences\n",
    "# for i in range(10):\n",
    "#     sentence = generate_sentence(english_model_3gram)\n",
    "#     print(f\"{i+1}. {sentence}\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: Creative Generation Experiments (Creativity Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Creative experiments (for creativity bonus)\n",
    "#\n",
    "# Ideas:\n",
    "# - Generate with different seed words\n",
    "# - Try different sampling strategies (greedy, random, top-k)\n",
    "# - Generate longer/shorter sentences\n",
    "# - Compare generation quality metrics\n",
    "# - Generate in your second language and compare\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation: Analysis and Observations\n",
    "\n",
    "**[YOUR ANALYSIS HERE]**\n",
    "\n",
    "Discuss your findings:\n",
    "- How did 2-gram and 3-gram generated sentences compare?\n",
    "- Which model produced more coherent sentences? Why?\n",
    "- What patterns did you notice in the generated text?\n",
    "- What were common errors or awkward constructions?\n",
    "- How does the quality of generated text relate to your corpus size and characteristics?\n",
    "- What are the fundamental limitations of n-gram generation?\n",
    "- How might modern neural language models (like GPT) improve upon these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 5: Reflection and Insights (15 points)\n",
    "\n",
    "Provide a comprehensive reflection on your experiments.\n",
    "\n",
    "**Baseline (10 pts):**\n",
    "- Key findings and surprises\n",
    "- Challenges encountered\n",
    "- Lessons learned about n-grams and language modeling\n",
    "- Limitations of the approaches used\n",
    "\n",
    "**Creativity Bonus (5 pts):**\n",
    "- Connections to linguistic theory or NLP concepts\n",
    "- Practical applications and implications\n",
    "- Suggestions for improvements\n",
    "- Insights about the relationship between model complexity and performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Reflection\n",
    "\n",
    "**[YOUR COMPREHENSIVE REFLECTION HERE]**\n",
    "\n",
    "Address the following points:\n",
    "\n",
    "### Key Findings\n",
    "- What were your main discoveries from this assignment?\n",
    "- What surprised you most?\n",
    "- How did your results compare to your initial expectations?\n",
    "\n",
    "### Technical Challenges\n",
    "- What were the most difficult aspects of implementation?\n",
    "- What debugging or problem-solving strategies did you use?\n",
    "- How did you handle edge cases or unexpected behaviors?\n",
    "\n",
    "### Model Performance\n",
    "- How did different n-gram sizes affect performance?\n",
    "- Why do you think certain models performed better than others?\n",
    "- What role did corpus size and quality play?\n",
    "\n",
    "### Limitations\n",
    "- What are the fundamental limitations of n-gram models?\n",
    "- What kinds of linguistic phenomena can't n-grams capture?\n",
    "- When would you choose (or not choose) n-grams for a real application?\n",
    "\n",
    "### Practical Applications\n",
    "- Where could these techniques be applied in real-world NLP systems?\n",
    "- How do n-gram models compare to modern neural approaches?\n",
    "- What are the trade-offs (accuracy, speed, resources, interpretability)?\n",
    "\n",
    "### Learning Outcomes\n",
    "- What did you learn about NLP and language modeling?\n",
    "- How has this assignment changed your understanding of how language models work?\n",
    "- What connections did you make to concepts from lectures or readings?\n",
    "\n",
    "### Future Directions\n",
    "- If you had more time, what would you explore further?\n",
    "- What improvements could you make to your implementation?\n",
    "- What other NLP techniques would you like to combine with n-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pre-Submission Checklist ‚úì\n",
    "\n",
    "Before submitting, verify that you have:\n",
    "\n",
    "### Content Completeness\n",
    "- [ ] Filled in name and student ID at the top\n",
    "- [ ] Completed all baseline requirements for Tasks 1-5\n",
    "- [ ] Written analysis and observations in all required sections\n",
    "- [ ] Included at least some creativity bonus elements\n",
    "\n",
    "### Technical Requirements  \n",
    "- [ ] All code cells execute without errors\n",
    "- [ ] Ran all cells: `Runtime ‚Üí Run all`\n",
    "- [ ] All outputs (tables, plots, text) are visible\n",
    "- [ ] Code is well-commented and readable\n",
    "\n",
    "### Submission Files\n",
    "- [ ] Exported notebook to PDF: `File ‚Üí Print ‚Üí Save as PDF`\n",
    "- [ ] Downloaded .ipynb file: `File ‚Üí Download ‚Üí Download .ipynb`\n",
    "- [ ] Checked PDF rendering (all content visible)\n",
    "- [ ] Verified file names: `FirstnameLastname_StudentID_as2.pdf` and `.ipynb`\n",
    "- [ ] Ready to upload both files to ODTUClass\n",
    "\n",
    "### Quality Check\n",
    "- [ ] Results make sense and are properly interpreted\n",
    "- [ ] Writing is clear and free of major errors\n",
    "- [ ] Visualizations are labeled and readable\n",
    "- [ ] Citations included if you used external resources\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "\n",
    "**Academic Integrity:**\n",
    "- This is individual work - do not copy from others\n",
    "- You may discuss concepts with classmates but write your own code and analysis\n",
    "- Cite any external resources you consulted (tutorials, Stack Overflow, etc.)\n",
    "- Using AI assistants is allowed for debugging but not for generating complete solutions\n",
    "\n",
    "**Getting Help:**\n",
    "- Review lecture materials and readings\n",
    "- Attend office hours\n",
    "- Ask questions on the course forum\n",
    "- Start early to allow time for troubleshooting\n",
    "\n",
    "**Good luck! üéì**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
